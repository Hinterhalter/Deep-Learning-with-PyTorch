{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import GloVe, FastText, CharNGram\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torchtext.datasets.imdb import IMDB\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, fix_length=200, batch_first=True)\n",
    "LABEL = data.Field(sequential=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300), max_size=10000, min_freq=10)\n",
    "LABEL.build_vocab(train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=32, device=-1)\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(TEXT.vocab)\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBCnn(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, hidden_size, n_cat, bs=1, kernel_size=3, max_len=200):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = bs\n",
    "        self.e = nn.Embedding(n_vocab, hidden_size)\n",
    "        self.cnn = nn.Conv1d(max_len, hidden_size, kernel_size)\n",
    "        self.avg = nn.AdaptiveAvgPool1d(10)\n",
    "        self.fc = nn.Linear(1000, n_cat)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        bs = inp.size()[0]\n",
    "        if bs != self.bs:\n",
    "            self.bs = bs\n",
    "        e_out = self.e(inp)\n",
    "        cnn_o = self.cnn(e_out)\n",
    "        cnn_avg = self.avg(cnn_o)\n",
    "        cnn_avg = cnn_avg.view(self.bs,-1)\n",
    "        fc = F.dropout(self.fc(cnn_avg), p=0.5)\n",
    "        return self.softmax(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IMDBCnn(n_vocab, hidden_size, n_cat=3, bs=32, kernel_size=2)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def fit(epoch, model, data_loader, phase='training', volatile=False):\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if phase == 'validation':\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        text, target = batch.text, batch.label\n",
    "        \n",
    "        if is_cuda:\n",
    "            text, target = text.cuda(), target.cuda()\n",
    "        \n",
    "        if phase=='training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        running_loss += F.nll_loss(output, target, size_average=False).data\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        \n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100. * running_correct.item() / len(data_loader.dataset)\n",
    "    \n",
    "    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n",
    "    \n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joshua\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is  0.84 and training accuracy is 11811/25000     47.24\n",
      "validation loss is  0.75 and validation accuracy is 13324/25000      53.3\n",
      "training loss is  0.67 and training accuracy is 14180/25000     56.72\n",
      "validation loss is  0.69 and validation accuracy is 14173/25000     56.69\n",
      "training loss is  0.59 and training accuracy is 15112/25000     60.45\n",
      "validation loss is  0.66 and validation accuracy is 14500/25000      58.0\n",
      "training loss is  0.55 and training accuracy is 15477/25000     61.91\n",
      "validation loss is   0.7 and validation accuracy is 14228/25000     56.91\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "\n",
    "for epoch in range(1,5):\n",
    "    \n",
    "    epoch_loss, epoch_accuracy = fit(epoch, model, train_iter, phase='training')\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(epoch, model, test_iter, phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
